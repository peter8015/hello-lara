[toc]

> 说明：包含微服务和云原生相关所有内容，理论+开源框架  spring boot, spring cloud, spring cloud alibaba
>
> 

# 什么是微服务

马丁福勒（Martin Fowler）：就目前而言，对于微服务业界并没有一个统一的、标准的定义。但通常而言，微服务架构是一种架构模式或者说是架构风格，它提倡将单一应用程序划分成一组小的服务。每个服务运行在其独立的自己的进程中服务之间相互配合、相互协调，为用户提供最终价值。服务之间采用轻量级通信。每个服务都围绕具体业务进行构建，并能够独立部署到生产环境等。另外应尽量避免统一的、集中的服务管理机制。

# 微服务的优缺点

优点：松耦合，聚焦单一业务功能，无关开发语言，团队规模降低。在开发中，不需要了解多有业务，只专注于当前功能，便利集中，功能小而精。微服务一个功能受损，对其他功能影响并不是太大，可以快速定位问题。微服务只专注于当前业务逻辑代码，不会和 html、css 或其他界面进行混合。可以灵活搭配技术，独立性比较舒服。

缺点：随着服务数量增加，管理复杂，部署复杂，服务器需要增多，服务通信和调用压力增大，运维工程师压力增大，人力资源增多，系统依赖增强，数据一致性，性能监控。

# 微服务优势？

- 快速迭代+快速回滚

- 资源利用大大提高

- 大幅度降低成本

- 高可用
  - 弹性机房水位调拨
  - 流量整形：根据每个微服务承载能力的不同，控制外部流量抵达服务的速率。“限流”其实只是流量整形的一个场景，大型微服务的流量整形有很多种方式，比如匀速排队、流量预热、削峰填谷等等。
  - 熔断降级
  
- 微服务架构是通过应用领域模型等理论，将庞大的单体应用拆分为更细粒度的小型服务，每个服务都可以独立部署、测试和发布，加之敏捷开发的推广，使得微服务很好地迎合了如今互联网行业快速试错、快速迭代的节奏，同时也保证了系统的可用性。


# 微服务范围？

两个特点：高可用、高扩展
三大功能：

- 服务通信：服务治理、服务间调用、
- 容错排障：流量整形、调用链分析、
- 分布式能力建设：服务网关、配置管理、消息驱动

# spring cloud简化架构图



![image-20220528172257585](https://i0.wp.com/tva1.sinaimg.cn/large/e6c9d24ely1h56600uf7aj21180seq50.jpg)

在上面这幅图中，我们可以看到有几个 Spring Boot Apps 的应用集群，这就是经过拆分后的微服务。Spring Cloud 和 Spring Boot 达成了一种默契的配合：Spring Boot 主内，通过自动装配和各种开箱即用的特性，搞定了数据层访问、RESTful 接口、日志组件、内置容器等等基础功能，让开发人员不费吹灰之力就可以搭建起一个应用；Spring Cloud 主外，在应用集群之外提供了各种分布式系统的支持特性，帮助你轻松实现负载均衡、熔断降级、配置管理等诸多微服务领域的功能。

# spring cloud netflix  vs spring cloud alibaba

![image-20220528172901907](/Users/haibingzhang/d/weiyundata/typoradoc/images/e6c9d24ely1h2o8yykwpxj214i0smq6h.jpg)

## 总体介绍    spring cloud alibaba生态包含什么？



![image-20220528204858179](https://i0.wp.com/tva1.sinaimg.cn/large/e6c9d24ely1h2oeqygrkpj21cy0u0n1o.jpg)




## 总结
在整个项目中，我们先通过 Spring Boot 快速落地了优惠券平台的三个业务模块，然后，在 Spring Cloud 实战阶段，我们分为三个阶段对 Spring Boot 项目进行微服务化改造：
- 第一个阶段使用 Nacos、Loadbalancer 和 OpenFeign 实现了跨服务的调用；
- 第二阶段使用 Sentinel、Nacos Config 和 Sleuth 实现了服务容错、配置管理和分布式链路追踪；
- 第三阶段使用 Gateway、Stream 和 Seata 实现了微服务网关、消息事件驱动和分布式事务。

## 第一阶段
- 服务治理：服务治理的重点是搭建基础的跨服务调用功能。我会把用户服务、优惠计算服务和订单服务改造成可以独立启动的微服务，并借助 Nacos 的服务发现功能，通过 Webflux 组件中的 WebClient 实现基于 HTTP 的跨服务间的调用；
- 负载均衡：在这部分，我们将在服务治理的基础上，引入 Loadbalancer 组件为跨服务调用添加负载均衡的能力。除此之外，我会对 Loadbalancer 组件的扩展接口做自定义开发，实现一个金丝雀测试的负载均衡场景；
- 简化服务调用：我将使用 OpenFeign 组件对用户服务进行改造，将原先复杂的 WebClient 调用替换为简洁的 OpenFeign 调用。

## 第二阶段

实战重点有三个：
- 利用服务容错提高微服务架构的可用性；
- 搭建全链路的分布式链路追踪能力；
- 实现统一的配置管理和动态属性推送。

这个阶段涉及的技术组件是 Nacos Config、Sentinel、Sleuth+Zipkin+ELK。

- 在微服务架构中，服务容错是保障服务高可用的一个重要手段。在这个项目中，我们选择用 Sentinel 作为服务容错组件，它也是 Alibaba 贡献给 Spring Cloud 的。Sentinel 秉承了阿里系“大而全”的传统，只这一款组件就可以实现降级、熔断、流量整形等多种服务容错途径。

- 链路追踪也是微服务架构中一个很重要的功能，线上异常排查全靠它提供线索。我使用了 Spring Cloud 官方开源的 Sleuth 实现了日志打标功能，使用全局唯一标记将一次跨微服务调用链上的各个环节全部串联起来。

- 光打标还没用，我还结合了 Zipkin 组件实现调用链的可视化检索，将调用链上各个阶段的请求按顺序显示在页面上，这样，我们就可以一目了然定位到线上异常发生在哪个环节。另外，我使用了目前业界主流的 ELK 组合（Elastic Search + Logstash + Kibana）作为日志检索系统。

实现这些能力：

配置管理：配置管理的重点是将三个微服务应用接入到 Nacos Config 配置中心，使用远程配置中心存储部分配置项。
服务容错：搭建 Sentinel Dashboard 控制台，通过控制台将降级规则和流量整形规则应用到业务埋点中。
链路追踪：这部分的重点是搭建分布式链路追踪与日志系统。

## 第三阶段

在第三阶段，我们的实战重点有三个：
- 搭建微服务网关作为统一流量入口；
- 使用消息驱动组件对接 RabbitMQ；
- 通过分布式事务保证数据一致性。
这个阶段涉及的技术组件是 Gateway、Stream 和 Seata。

- `微服务网关`是架设在外部网关（如 Ngnix）和内部微服务之间的一座桥梁，我选用 Spring Cloud Gateway 作为网关组件。Gateway 不光担任了路由转发的重任，同时它提供了丰富的谓词组合实现复杂的路由判断逻辑。除此以外，你还可以在网关层定义拦截器，对来访请求执行一段特殊的业务逻辑。曾经微服务网关的头把交椅是 Netflix 贡献的 Zuul 组件，但 Zuul 2.0 的开源发布一拖再拖，且性能并未达到预期效果。Spring Cloud 官方迫不得已，还没等到 Zuul 2.0 发布，就自己发布了一款开源网关组件 Spring Cloud Gateway。基于这些原因，Gateway 当之无愧成为了网关层的不二选择。
- `消息队列和消息驱动`是老牌技术了，它并不是微服务特有的功能，我之所以在课程中加入了消息驱动这个内容，主要有两个原因。一是我想让你了解 Spring Cloud 开源的消息驱动组件“Stream”，它可以大幅降低应用系统和消息组件之间的对接流程。二是消息组件在如今有非常丰富的使用场景，我希望将“消息组件的应用场景”作为一个知识拓展点，帮助你开阔眼界。
- `分布式事务`是微服务环境下保证事务一致性的终极手段。在课程中我将主要介绍两种比较有代表性的 Seata 分布式事务解决方案，一种是没有代码侵入的 Seata AT 方案，另一种是蚂蚁金服贡献的资源锁定 + 补偿型的 Seata TCC 方案。





# spirng cloud alibaba - nacos

## 集群搭建

单机启动：
bash startup.sh -m standalone





## Nacos 自动装配原理





# spring cloud load balancer

## 为什么需要负载均衡？

需要把流量分配到各个服务器集群上，提高服务端的处理能力。


## 网关层负载均衡（服务端）特点及优缺点？

客户端发起服务调用时，服务请求并不是直接发送到目标服务器，而是发送到网关这个全局的负载均衡。网关再根据配置的负载策略将服务请求转发到目标服务器。

- 优点：网关层的负载均衡应用比较广泛，它不依赖于服务发现技术。并且客户端不需要拉取服务列表，也并不需要关心采用的负载策略。
- 缺点: 
  - 增加了网络消耗：增加了一次客户端请求网关的网络调用，在高并发情况下会增加10~20ms调用时间，在超高qps场景下影响系统吞吐量。
  - 复杂度和故障率提升：需要搭建额外的网关组件来支持负载均衡，增加了系统的复杂度。多出来的这次网络调用也会增加请求的失败率。

## 客户端负载均衡特点及优缺点？

客户端会拉取服务列表到本地，根据本地配置的负载均衡策略，直接调用目标服务器。
优点：网络开销小，由客户端直接发起的点到点的调用；配置灵活，各个客户端可以根据自己的情况灵活配置负载策略。
缺点：客户端需要拉取服务列表，依赖于服务发现技术。spring cloud load balancer + nacos

## 负载均衡的分类//-todo

- 硬件负载均衡：F5
- 软件负载均衡：Dns负载均衡、nginx软负载均衡


## spring cloud load balancer工作原理？

依赖于@LoadBalancer注解，这个注解在启动时利用自动装配机制会自动的向webclient中加入一个特殊的filter，通过这个过滤器实现了负载均衡的功能。

- 第一步：声明负载均衡过滤器。ReactorLoadBalancerClientAutoConfiguration是一个自动装配器类，它会初始化一个ExchangeFilterFunction的实例，会作为过滤器被注入到webclient中。
- 第二步：声明后置处理器。LoadBalancerBeanPostProcessorAutoConfiguration这个类会在第一步创建的实例后添加一个后置处理器（LoadBalancerWebClientBuilderBeanPostProcessor）
- 第三步：添加过滤器到webclient。LoadBalancerWebClientBuilderBeanPostProcessor 后置处理器开始发挥作用，将过滤器添加到 WebClient 中实现负载均衡。

## 如何深度定制load balancer实现自己的负载均衡策略？

load balancer提供了两种负载均衡策略：

- 轮询策略randomRobinLoadBalancer：通过内部保存一个position计数器，按照从上到下依次调用服务，每调用后计数器加1.
- 随机策略randomLoadBalancer：从服务列表中随机选择一台服务器进行调用。

金丝雀测试：
金丝雀测试是灰度测试的一种。线上测试一个功能改动，在一个极小的范围内测试。可以部署改动到集群中的几台机器，带“测试流量标记的”请求会打到这几台机器上，这几台机器称为“金丝雀”。正常流量还是打到正常服务器，不影响线上环境。

- 编写自定义负载均衡策略 canaryRule
  创建了一个叫 CanaryRule 的负载均衡规则类，它继承自 Loadbalancer 项目的标准接口
-  ReactorServiceInstanceLoadBalancer。CanaryRule 借助 Http Header 中的属性和 Nacos 服务节点的 metadata 完成测试流量的负载均衡。在这个过程里，它需要准确识别哪些请求是测试流量，并且把测试流量导向到正确的目标服务。
    - CanaryRule 如何识别测试流量：如果发出一个webclient的请求，根据header列表 中包含了特定流量key：traffic-verion，那么这个请求被识别成一个特殊请求，被转发到金丝雀服务器上
    - CanaryRule 如何对测试流量做负载均衡：新的代码改动的版本会被部署到金丝雀服务器上，在nacos元数据中对这些服务器进行打标，如果nacos元数据中的"traffic-version"与测试流量header中的一致，那么请求就会被转发到金丝雀服务器上。
- 配置负载均衡策略： 声明一个CanaryRuleConfiguration注入到容器，注意不是全局（不要加@AutoConfiguration注解），写好配置类后，将客户端加上注解 @LoadBalancer(configuration=CanaryRuleConfiguration.class)
- 测试流量打标: 测试流量打标方法很多：可以在header中加入特殊的key-value，在rpc context中放一个特殊值，或是在request中加一个成员变量。
- 添加nacos元数据：在nacos控制台实例的详情中加入元数据，如“traffic-version=test-001”



# spring cloud openfeign

> OpenFeign 组件的前身是 Netflix Feign 项目，它最早是作为 Netflix OSS 项目的一部分，由 Netflix 公司开发。后来 Feign 项目被贡献给了开源组织，于是才有了我们今天使用的 Spring Cloud OpenFeign 组件。

OpenFeign 提供了一种声明式的远程调用接口，它可以大幅简化远程调用的编程体验。

``` java
String response = helloWorldService.hello("Vincent Y.");
```

## OpenFeign如何实现远程调用？

> OpenFeign使用了“动态代理技术”技术来封装远程过程调用过程 。

``` java

@FeignClient(value = "hello-world-serv")
public interface HelloWorldService {

    @PostMapping("/sayHello")
    String hello(String guestName);
}
```



openFeign会生成 一个代理 类，对所有通过接口发起的远程调用生成动态 代理 。

调用流程：

- 首先，在项目启动阶段，OpenFeign 框架会发起一个主动的扫包流程，从指定的目录下扫描并加载所有被 @FeignClient 注解修饰的接口。
- 然后，OpenFeign 会针对每一个 FeignClient 接口生成一个动态代理对象，即图中的 FeignProxyService，这个代理对象在继承关系上属于 FeignClient 注解所修饰的接口的实例。
- 接下来，这个动态代理对象会被添加到 Spring 上下文中，并注入到对应的服务里，也就是图中的 LocalService 服务。最后，LocalService 会发起底层方法调用。实际上这个方法调用会被 OpenFeign 生成的代理对象接管，由代理对象发起一个远程服务调用，并将调用的结果返回给 LocalService。	

## OpenFeign是如何通过代理技术创建对象的？

![image-20220611112323708](/Users/haibingzhang/d/weiyundata/typoradoc/images/image-20220611112323708.png)

- 项目加载：在项目的启动阶段，EnableFeignClients 注解扮演了“启动开关”的角色，它使用 Spring 框架的 Import 注解导入了 FeignClientsRegistrar 类，开始了 OpenFeign 组件的加载过程。

- 扫包：FeignClientsRegistrar 负责 FeignClient 接口的加载，它会在指定的包路径下扫描所有的 FeignClients 类，并构造 FeignClientFactoryBean 对象来解析 FeignClient 接口。
- 解析 FeignClient 注解：FeignClientFactoryBean 有两个重要的功能，一个是解析 FeignClient 接口中的请求路径和降级函数的配置信息；另一个是触发动态代理的构造过程。其中，动态代理构造是由更下一层的 ReflectiveFeign 完成的。
- 构建动态代理对象：ReflectiveFeign 包含了 OpenFeign 动态代理的核心逻辑，它主要负责创建出 FeignClient 接口的动态代理对象。ReflectiveFeign 在这个过程中有两个重要任务，一个是解析 FeignClient 接口上各个方法级别的注解，将其中的远程接口 URL、接口类型（GET、POST 等）、各个请求参数等封装成元数据，并为每一个方法生成一个对应的 MethodHandler 类作为方法级别的代理；另一个重要任务是将这些 MethodHandler 方法代理做进一步封装，通过 Java 标准的动态代理协议，构建一个实现了 InvocationHandler 接口的动态代理对象，并将这个动态代理对象绑定到 FeignClient 接口上。这样一来，所有发生在 FeignClient 接口上的调用，最终都会由它背后的动态代理对象来承接。



















# springboot



## 参考：

- springboot面试题：

​		https://blog.csdn.net/qq_27184497/article/details/117886379

​		[面试题](https://blog.csdn.net/weixin_44772609/article/details/115106450?spm=1001.2101.3001.6650.7&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-7.pc_relevant_paycolumn_v3&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-7.pc_relevant_paycolumn_v3&utm_relevant_index=14)

- blog：

​		[Spring Boot 2.x基础教程](https://blog.didispace.com/spring-boot-learning-2x/)

​		[springboot中文blog](http://springboot.fun/ "springboot")



## spring boot启动过程

> https://www.bilibili.com/video/BV1sA4y1X7h9?p=30&spm_id_from=pageDriver&vd_source=d6b96f6ee0f0d1cdf5aee8522b5cdb9f
>
> https://dalin.blog.csdn.net/article/details/122505264?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-122505264-blog-117886379.pc_relevant_antiscanv2&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-1-122505264-blog-117886379.pc_relevant_antiscanv2&utm_relevant_index=1



- 启动过程 

  ![image](https://i0.wp.com/tva1.sinaimg.cn/large/e6c9d24ely1h565zpmgroj20oj0umaeo.jpg)

- 源码

![image-20220624161918526](https://i0.wp.com/tva1.sinaimg.cn/large/e6c9d24ely1h3jetf7oo1j20yq0cc0u6.jpg)



我们引入starter的依赖，会将自动配置的类的jar引入。@SpringBootApplication的注解中有一个是@EnableAutoConfiguration注解，这个注解有一个@Import({EnableAutoConfigurationImportSelector.class})，EnableAutoConfigurationImportSelector内部则是使用了SpringFactoriesLoader.loadFactoryNames方法进行扫描具有META-INF/spring.factories文件的jar包。而自动配置类的jar的是有一个META-INF/spring.factories文件内容如下：

（http://www.51gjie.com/javaweb/1041.html）

创建自己的starter：https://blog.csdn.net/zhaohong_bo/article/details/89924053



## 源码解析：

``` java
public SpringApplication(ResourceLoader resourceLoader, Class<?>... primarySources) {
		this.resourceLoader = resourceLoader;
		Assert.notNull(primarySources, "PrimarySources must not be null");
		this.primarySources = new LinkedHashSet<>(Arrays.asList(primarySources));
    //确定应用程序的类型：none, reactive,  servlet(WebApplicationType.SERVLET)
		this.webApplicationType = WebApplicationType.deduceFromClasspath();
    //加载所有的初始化器
		setInitializers((Collection) getSpringFactoriesInstances(
				ApplicationContextInitializer.class));
    //加载所有的监听器
		setListeners((Collection) getSpringFactoriesInstances(ApplicationListener.class));
    //设置应用程序的主类
		this.mainApplicationClass = deduceMainApplicationClass();
	}

```



## 扩展点：

> https://www.cnblogs.com/hafiz/p/9155017.html   
>
> https://zhuanlan.zhihu.com/p/266126121
>
> 一般情况下不需要扩展

- 当然，我们也可以自己实现一个自定义的初始化器：实现 ApplicationContextInitializer接口既可

  ``` java
  package com.spring.application;
   
  import org.springframework.context.ApplicationContextInitializer;
  import org.springframework.context.ConfigurableApplicationContext;
  /**
   * 自定义的初始化器
   */
  public class MyApplicationContextInitializer implements ApplicationContextInitializer<ConfigurableApplicationContext> {
      @Override
      public void initialize(ConfigurableApplicationContext configurableApplicationContext) {
          System.out.println("我是初始化的 MyApplicationContextInitializer...");
      }
  }
  ```

- 执行自定义的run方法
  这是一个扩展功能，callRunners(context, applicationArguments) 可以在启动完成后执行自定义的run方法；有2中方式可以实现：

  实现 ApplicationRunner 接口
  实现 CommandLineRunner 接口

  ``` java
  package com.spring.init;
   
  import org.springframework.boot.ApplicationArguments;
  import org.springframework.boot.ApplicationRunner;
  import org.springframework.boot.CommandLineRunner;
  import org.springframework.stereotype.Component;
   
  /**
   * 自定义run方法的2种方式
   */
  @Component
  public class MyRunner implements ApplicationRunner, CommandLineRunner {
   
      @Override
      public void run(ApplicationArguments args) throws Exception {
          System.out.println(" 我是自定义的run方法1，实现 ApplicationRunner 接口既可运行"        );
      }
   
      @Override
      public void run(String... args) throws Exception {
          System.out.println(" 我是自定义的run方法2，实现 CommandLineRunner 接口既可运行"        );
      }
  }
  ```

- 如何实现自己的自动配置类

  - 编写java config   @Configuration

  - 添加条件   @Conditional

  - 定位自动配置spring.factories

    

# 参考

https://zhuanlan.zhihu.com/p/41228196
https://zhuanlan.zhihu.com/p/130332285
https://blog.csdn.net/b6ecl1k7BS8O/article/details/86653449









# 云原生架构：

>
>视频：https://www.bilibili.com/video/BV13Q4y1C7hS?p=29&vd_source=d6b96f6ee0f0d1cdf5aee8522b5cdb9f
>
>https://www.yuque.com/leifengyang/oncloud/ctiwgo
>

//todo    如何做日志追踪 

- 云原生架构师的职位要求：（京东云）

  - 掌握主流微服务架构设计及技术（如SpringCloud体系下的微服务治理），有Java开发或技术运维经验；

  - 熟悉主流中间件的最佳实践路径（mysql、redis集群、分片等）

  - 5年以上云计算从业工作经历，具备有大中型公有云/私有云/混合云项目的系统架构设计与项目实施经验优先；

  - 结合京东云产品，为客户提供安全可靠的云原生技术方案，包括但不限于容灾方案设计、高可用技术架构设计等

  - 结合客户业务对客户业务上云进行方案设计及落地指导

  - 掌握如Rancher，Kubesphere或其他容器云管理平台之一的使用优先；

  - 熟悉企业典型组网、业界主流中间件技术、主流数据库技术架构等；

    



问题：

1. 京东云的产品有什么？
1. 云原生的技术解决方案指什么？实际案例？经常遇到的问题？
2. 容灾方案如何设计？实际案例？经常遇到的问题？
3. 云项目的系统架构设计与项目实施经验优先
4. 微服务架构设计及技术（SpringCloud）
5. 主流中间件的最佳实践路径（mysql、redis集群、分片等）
6. Kubesphere架构、实践、问题
7. 云进行方案设计及落地指导

- k8s 集群架构

![](https://p.ipic.vip/j6jc34.png)

### kube-apiserver[ ](https://kubernetes.io/zh-cn/docs/concepts/overview/components/#kube-apiserver)

API 服务器是 Kubernetes [控制平面](https://kubernetes.io/zh-cn/docs/reference/glossary/?all=true#term-control-plane)的组件， 该组件负责公开了 Kubernetes API，负责处理接受请求的工作。 API 服务器是 Kubernetes 控制平面的前端。

Kubernetes API 服务器的主要实现是 [kube-apiserver](https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-apiserver/)。 `kube-apiserver` 设计上考虑了水平扩缩，也就是说，它可通过部署多个实例来进行扩缩。 你可以运行 `kube-apiserver` 的多个实例，并在这些实例之间平衡流量。

### etcd[ ](https://kubernetes.io/zh-cn/docs/concepts/overview/components/#etcd)

`etcd` 是兼顾一致性与高可用性的键值数据库，可以作为保存 Kubernetes 所有集群数据的后台数据库。

你的 Kubernetes 集群的 `etcd` 数据库通常需要有个[备份](https://kubernetes.io/zh-cn/docs/tasks/administer-cluster/configure-upgrade-etcd/#backing-up-an-etcd-cluster)计划。

如果想要更深入的了解 `etcd`，请参考 [etcd 文档](https://etcd.io/docs/)。

### kube-scheduler

`kube-scheduler` 是[控制平面](https://kubernetes.io/zh-cn/docs/reference/glossary/?all=true#term-control-plane)的组件， 负责监视新创建的、未指定运行[节点（node）](https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/)的 [Pods](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/)， 并选择节点来让 Pod 在上面运行。

调度决策考虑的因素包括单个 Pod 及 Pods 集合的资源需求、软硬件及策略约束、 亲和性及反亲和性规范、数据位置、工作负载间的干扰及最后时限。

### kube-controller-manager

[kube-controller-manager](https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-controller-manager/) 是[控制平面](https://kubernetes.io/zh-cn/docs/reference/glossary/?all=true#term-control-plane)的组件， 负责运行[控制器](https://kubernetes.io/zh-cn/docs/concepts/architecture/controller/)进程。

从逻辑上讲， 每个[控制器](https://kubernetes.io/zh-cn/docs/concepts/architecture/controller/)都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在同一个进程中运行。

这些控制器包括：

- 节点控制器（Node Controller）：负责在节点出现故障时进行通知和响应
- 任务控制器（Job Controller）：监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成
- 端点控制器（Endpoints Controller）：填充端点（Endpoints）对象（即加入 Service 与 Pod）
- 服务帐户和令牌控制器（Service Account & Token Controllers）：为新的命名空间创建默认帐户和 API 访问令牌



### cloud-controller-manager

`cloud-controller-manager` 是指嵌入特定云的控制逻辑之 [控制平面](https://kubernetes.io/zh-cn/docs/reference/glossary/?all=true#term-control-plane)组件。 `cloud-controller-manager` 允许你将你的集群连接到云提供商的 API 之上， 并将与该云平台交互的组件同与你的集群交互的组件分离开来。

`cloud-controller-manager` 仅运行特定于云平台的控制器。 因此如果你在自己的环境中运行 Kubernetes，或者在本地计算机中运行学习环境， 所部署的集群不需要有云控制器管理器。

与 `kube-controller-manager` 类似，`cloud-controller-manager` 将若干逻辑上独立的控制回路组合到同一个可执行文件中， 供你以同一进程的方式运行。 你可以对其执行水平扩容（运行不止一个副本）以提升性能或者增强容错能力。

下面的控制器都包含对云平台驱动的依赖：

- 节点控制器（Node Controller）：用于在节点终止响应后检查云提供商以确定节点是否已被删除
- 路由控制器（Route Controller）：用于在底层云基础架构中设置路由
- 服务控制器（Service Controller）：用于创建、更新和删除云提供商负载均衡器



## Node 组件

节点组件会在每个节点上运行，负责维护运行的 Pod 并提供 Kubernetes 运行环境。

### kubelet

`kubelet` 会在集群中每个[节点（node）](https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/)上运行。 它保证[容器（containers）](https://kubernetes.io/zh-cn/docs/concepts/overview/what-is-kubernetes/#why-containers)都运行在 [Pod](https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/) 中。

kubelet 接收一组通过各类机制提供给它的 PodSpecs， 确保这些 PodSpecs 中描述的容器处于运行状态且健康。 kubelet 不会管理不是由 Kubernetes 创建的容器。

### kube-proxy

[kube-proxy](https://kubernetes.io/zh-cn/docs/reference/command-line-tools-reference/kube-proxy/) 是集群中每个[节点（node）](https://kubernetes.io/zh-cn/docs/concepts/architecture/nodes/)所上运行的网络代理， 实现 Kubernetes [服务（Service）](https://kubernetes.io/zh-cn/docs/concepts/services-networking/service/) 概念的一部分。

kube-proxy 维护节点上的一些网络规则， 这些网络规则会允许从集群内部或外部的网络会话与 Pod 进行网络通信。

如果操作系统提供了可用的数据包过滤层，则 kube-proxy 会通过它来实现网络规则。 否则，kube-proxy 仅做流量转发。



![image-20220721094908003](https://tva1.sinaimg.cn/large/e6c9d24ely1h565zbnsiqj21f00sk77x.jpg)





一站式devops解决方案：	

![image-20220722000250966](https://tva1.sinaimg.cn/large/e6c9d24ely1h565z5xvztj21j80u0n36.jpg)





![image-20220722000400727](/Users/haibingzhang/d/weiyundata/typoradoc/images/image-20220722000400727.png)

![image-20220722000429425](/Users/haibingzhang/d/weiyundata/typoradoc/images/image-20220722000429425.png)







![image-20220725112607081](/Users/haibingzhang/d/weiyundata/typoradoc/images/image-20220725112607081.png)





# istio 结合k8s   

https://blog.csdn.net/qq_39578545/article/details/117668043







# docker

> docker有使用经验
>
> 一些面试题：   https://blog.csdn.net/shanghongshen/article/details/121603303





# sleuth面试题

1. Spring Cloud Sleuth 如何实现分布式追踪？
答：Sleuth通过在微服务调用链中插入追踪信息，主要包括Trace ID、Span ID和Baggage（携带数据）。每个服务请求都会生成一个唯一的Trace ID，并且每个服务内部的操作会被映射为一个或多个Span，Span之间通过Parent Span ID关联起来形成调用树。Sleuth利用拦截器在HTTP请求头中添加或读取B3协议规定的追踪信息，实现跨进程追踪上下文的传递。
2. 如何处理跨进程追踪信息的传递？
答：跨进程追踪信息主要是通过HTTP头部传递的，Sleuth默认使用的是B3协议的头部，包括X-B3-TraceId、X-B3-SpanId、X-B3-ParentSpanId等字段。服务间调用时，发起请求的服务将当前Span的相关信息放入请求头，接收请求的服务则从请求头中取出这些信息，新建或关联本地的Span。
3. Sleuth如何与Zipkin整合，上报追踪数据？
答：Sleuth集成了Zipkin追踪系统，通过ZipkinSpanReporter将本地收集到的Span信息报告给Zipkin服务器。用户可以配置Sleuth使用HTTP、Kafka、RabbitMQ等方式将Span数据上报。上报的内容包括Span的基本信息、时间戳、标签（tags）、注解（annotations）等，用于后续的可视化分析和性能监控。
4. Sleuth如何与Spring Cloud Stream或Ribbon等组件协同工作？
答：Sleuth通过适配器或者拦截器与Spring Cloud Stream、Ribbon等组件配合，确保无论是在消息队列还是RESTful API调用时都能正确地传递追踪上下文。例如，在Ribbon环境下，Sleuth会包装HTTP客户端，确保请求头带有正确的追踪信息。
5. 如何配置采样策略？
答：Sleuth允许用户配置采样策略，通过Sampler接口可以自定义何时开始新的Span以及是否将其记录到追踪系统中。例如，可以基于百分比采样，只记录一定比例的请求，减轻追踪系统的负担。在配置文件中可设置全局采样率。
6. 如何在日志中加入追踪信息？
答：Sleuth通过与SLF4J、Logback或Log4j等日志框架集成，为日志增加MDC（Mapped Diagnostic Context）信息，使得每条日志都能够携带追踪ID和Span ID，便于关联日志与具体请求。
7. 如何在Sleuth中手动创建和关闭Span？
答：在Sleuth中，可以通过Tracer接口创建Span，通常使用tracer.nextSpan()获取一个Span实例，调用.start()开始Span，并在适当的时候调用.finish()结束Span。还可以通过.withSpanInScope(span)方法将Span置于当前线程上下文中，确保该线程后续的任何操作都被记录到该Span中。
8. 如何处理异步调用场景下的追踪？
答：在异步调用场景下，Sleuth通过CurrentTraceContext接口管理追踪上下文的传递。在异步任务开始前，可以从CurrentTraceContext中获取当前Span并保存，异步任务执行时再恢复这个Span的上下文，确保即使跨越线程也能正确记录追踪信息。

## sleuth实现原理

1. 跟踪上下文封装 - Brave
    Sleuth 主要是基于 Brave 这个库来实现分布式追踪功能。Brave 封装了追踪上下文（trace context），这个上下文包含 Trace ID、Span ID、Sampling decision（采样决策）等信息。
    在 Brave 中，Tracer 类是追踪的核心，用于创建和管理 Span，TraceContext 类则代表追踪上下文，存储和传递追踪元数据。

  ``` java
  // Brave 中的 Tracer 示例
  brave.Tracer tracer = ...
  Span span = tracer.nextSpan().name("my-operation").start();
  
  try (Tracer.SpanInScope ws = tracer.withSpanInScope(span)) {
      // 在此范围内的代码将会记录到当前span中
      ...
  } finally {
      span.finish();
  }
  ```

2. HTTP 请求拦截与追踪上下文传递
Sleuth 对 HTTP 客户端和服务端请求进行了拦截，通过 HttpTracing 对象和 HttpClientRequestParser、HttpServerRequestParser 等解析器，从请求头中读取或写入追踪上下文。
对于 Spring Webflux 框架，Sleuth 通过 WebFluxSleuthOperators 类的 client() 和 server() 方法对 WebClient 和 ServerHttpRequest/Response 进行装饰，实现追踪上下文的传递。
对于 Spring MVC 框架，Sleuth 通过 TraceRequestInterceptor 和 TraceHandlerInterceptor 来拦截请求，设置和提取追踪上下文。
3. 日志增强
Sleuth 与 Logback、Log4j 等日志框架集成，通过 LoggingSampler、Slf4jScopeDecorator 等类将追踪上下文与日志记录相结合，使得每条日志都带上对应的 Trace 和 Span ID。

``` java
@Bean
public LoggingSampler loggingSampler() {
    return new LoggingSampler(new Sampler() {...});
}

@Bean
public Slf4jScopeDecorator sleuthSlf4jScopeDecorator() {
    return new Slf4jScopeDecorator();
}

```

4. 跨进程追踪信息传递
在跨进程调用时，Sleuth 会将追踪上下文转换成 HTTP 头部（如 B3 headers），在服务间传递。接收方通过解析这些头部重新构造追踪上下文，保持调用链路的一致性。
5. 收集与上报
Sleuth 默认集成了 Zipkin，通过 ZipkinSpanReporter 和 Zipkin Brave Reporter 把 Span 信息发送到 Zipkin 服务器进行收集和展示。

``` java
@Bean
public ZipkinSpanReporter zipkinSpanReporter() {
    return AsyncReporter.create(OkHttpSender.create(zipkinUrl));
}

@Bean
public Brave brave(Tracing.Builder tracingBuilder, CurrentTraceContext currentTraceContext) {
    return tracingBuilder.spanReporter(zipkinSpanReporter())
            .currentTraceContext(currentTraceContext)
            // ...其他配置
            .build();
}
```

综上所述，Sleuth 主要是通过 Brave 库在微服务间的调用链路中创建、管理和传递追踪上下文，并将其与日志系统和分布式追踪系统（如 Zipkin）紧密结合，从而实现在分布式环境下的请求追踪与性能监控。由于篇幅限制，这里只是简单介绍了部分原理，实际上 Sleuth 的实现细节更为丰富和完善。
