

[toc]

> 说明：  包括方法论总结，设计案例的分析。
>
> 目标：明确进行分析和设计的方法论，深入剖析案例。
>
> Tag：设计部分、Coding设计、系统设计、案例分析
>
> crokking the system design
>
> https://www.designgurus.io/course/grokking-the-system-design-interview
>
> https://mp.weixin.qq.com/s?__biz=MzIzNjY1OTk5OQ==&mid=2247484701&idx=1&sn=769dc1861cbc190dbd8f3eda46bb56e7&chksm=e8d5316ddfa2b87bde76aa6355dc71bf119e34accf5a1c22dd9a084c9cecd31bccdb71d78ba2&scene=21#wechat_redirect
>
> 
>
> https://blog.csdn.net/NXHYD/article/details/122612092
>
> https://www.jiuzhang.com/course/77/?source=rw
>
> facebook question:
>
> https://www.interviewkickstart.com/blog/facebook-technical-interview-questions-to-crack-the-facebook-coding-interview
>
> https://blog.csdn.net/NXHYD/article/details/106576210
>
> 案例：
>
> https://www.zhihu.com/question/63947513/answer/2272316638



# todo

1. coupang面试总结



# 设计原则

- 大型网站核心架构要素
  - ==稳定性（可用性、扩展性、伸缩性）、性能、安全性==
- 系统模块设计要素
  - ==易用性、扩展性、灵活性、低延迟、高容错、==
- 设计思想、设计原则、设计模式、编码规范、重构技巧
  - ==SOLID，DRY 原则、KISS 原则、YAGNI 原则、LOD 法则==
  - ==MVC、ORM==
  - ==编码规范：P3C、代码大全、整洁之道（20条）==
  - ==重构技巧：一些坏味道 重构==
  - cap、base
- 代码要素
  - ==易读、易扩展、易维护、易测试、简洁、灵活、可复用。==
- 设计模式
  - 创建型常用的有：==单例模式、工厂模式（工厂方法和抽象工厂）、建造者模式。==
  - 结构型常用的有：==代理模式、桥接模式、装饰者模式、适配器模式。==
  - 行为型常用的有：==观察者模式、模板模式、策略模式、职责链模式、迭代器模式、状态模式。==

# 系统设计的面试关注点

coding面试题

1. 梳理清楚流程及要求
2. 注意方法的设计：名称及参数实体
3. 注意与算法相结合，使过程可运行

> 参考coupang面试题

1. 分析系统流程，给出流程图
2. 找到系统分析的难点，给出解决方案及替代方案
3. 找到系统风险点，给出方案

#  如何进行系统设计（过程总结）

## 标准流程（crokking system design）

本文将介绍系统设计过程的**基本步骤**：需要注意对应的产出物，与知识点和设计原则相结合 

1. 需求澄清：解决的问题是什么？目标是什么？产出流程图及需求功能和非需求功能（领域划分）
2. 系统接口定义API：希望这个系统能做什么，产出接口文档。
3. 粗略估算: 估算一下规模（现有或未来），产出容量、性能等要求
4. 定义数据模型：理顺数据在模块中的流动，分区和管理，产出E-R图（领域模型）
5. 高级定义: 识别出端到端的各个模块，产出核心模块和类图 
6. 细节定义：对模块进行分析，产出不同的方案并进行tradeoff.
7. 解决瓶颈：找到风险，产出预案（非功能性设计）

英文：

1. 需求澄清（Requirments Clarification） 
2. 系统接口定义 （System Interface Definition）
3. 粗略估算 (Back-of-the-envelope Estimation )
4. 定义数据模型 （Defining Data Model）
5. 高级定义 (High Level Design)
6. 细节设计（Detailed Design）
7. 识别并解决瓶颈（Identifying and Resovling Bottlenecks）



How to design a system:

1. Firstly,  requirements clarification. You should understand the requirements. 

## DDD：



## 功能组件架构设计

- 项目背景、目标（立项项目、小工具、业务需求）
- 功能性需求分析--==产出业务流程图、用户用例、主要模块、MVP功能==   微服务DDD
  - ==分析手段==：通过流程框图、用户用例、TDD等进行分析。
  - ==MVP==：预想框架的样子，以及用户如何使用，设定目标有针对性的思考。（配置文件，spring aop）
  - ==业务流程==：用户请求--限流框架--规则读取--判断限流？--正常请求（拒绝请求）产出流程图及具体流程（正常流程、异常流程）
  - ==主要模块==：分析主要模块：提供限流规则配置和编程接口
- 非功能性需求分析：==易用、灵活、可扩展、低延时、高容错==
  - ==易用性==：规则配置简单、接口使用简单、提供多种限流算法、与spring集成方便
  - ==扩展性、灵活性==：扩展算法、支持不同的格式 、不同的数据源、低延时
  - ==容错性==：高度容错，不能影响接口本身的性能。
- 需求设计:
  - 类图、e-r图、时序图

# 多租户概念

- 多租户（Multi-tenancy）是一种软件架构设计模式，主要应用于云计算和SaaS（Software as a Service，软件即服务）领域。在多租户系统中，一个单一的实例或应用程序可以为多个不同的用户群（称为租户）提供服务，每个租户拥有独立的、定制化的数据空间，并且看起来像是在使用专属于自己的系统。
- 多租户的核心特点包括：
	- 资源隔离：尽管所有租户共享物理基础设施（如服务器、数据库等），但必须确保各租户之间的数据、配置以及运行环境相互隔离，以保证数据安全性和隐私性。
	- 资源共享：通过共享底层硬件及软件资源，能够显著降低运营成本，提高资源利用率，并简化管理和维护。
	- 定制化与个性化：多租户系统通常允许不同租户对界面、功能模块、权限设置等进行一定程度的个性化配置，同时保持核心业务逻辑的一致性。
	- 可扩展性：随着租户数量的增长，系统需要具备良好的可扩展性，能够平滑地处理更多用户的请求并存储更多的数据。
- 常见的多租户实现策略有：
	- 共享数据库、独立Schema：所有租户共享同一个数据库，但在数据库中通过Schema或其他方式区分不同租户的数据。
	- 独立数据库：每个租户都有独立的数据库实例，从而实现更强的数据隔离。
	- 混合模式：根据租户规模或需求，在同一系统中结合使用上述两种或多种模式。
	多租户技术的关键在于如何在保障资源有效利用的同时，确保各租户之间操作互不影响，数据安全性得到充分保障，并能满足不同租户的不同业务需求。

# 幂等需求分析实例

> 参考王争幂等框架设计

需求分析
场景：描述业务场景，具体问题：超时分三种
需求分析：

- 分析方法：画线、用例图、tdd

- 功能：幂等号
- 非功能
  - 易用性、功能性、容错性

需求设计

- 幂等正常流程
  消息接收：异常时进行重试，符合预期
  消息处理：业务异常、业务宕机、幂等框架异常
- 结果返回：异常时产生幂等号，可重试

功能实现

//todo 还需要具体梳理与实

# 案例一：限流框架的设计

> 参考：https://time.geekbang.org/column/article/242314

## 项目背景：

公司成立初期，团队人少。公司集中精力开发一个金融理财产品（我们把这个项目叫做 X 项目）。整个项目只做了简单的前后端分离，后端的所有代码都在一个 GitHub 仓库中，整个后端作为一个应用来部署，没有划分微服务。

遇到了行业风口，公司发展得不错，公司开始招更多人，开发更多的金融产品，比如专注房贷的理财产品、专注供应链的产品、专注消费贷的借款端产品等等。在产品形态上，每个金融产品都做成了独立的 App。对于不同的金融产品，尽管移动端长得不一样，但是后

端的很多功能、==代码都是可以复用的==。为了快速上线，针对每个应用，公司都成立一个新的团队，然后拷贝 X 项目的代码，在此基础之上修改、添加新的功能。

因为所有的项目的代码都是从 X 项目拷贝来的，==多个团队同时维护相似的代码==，显然是重复劳动，协作起来也非常麻烦。任何团队发现代码的 bug，都要同步到其他团队做相同的修改。而且，各个团队对代码独立迭代，改得==面目全非==，即便要添加一个通用的功能，每个团队也都要基于自己的代码再重复开发。

除此之外，公司成立初期，各个方面条件有限，只能招到开发水平一般的员工，而且追求快速上线，所以，X 项目的==代码质量很差，结构混乱、命名不规范、到处是临时解决方案==、埋了很多坑，在烂代码之上不停地堆砌烂代码，时间长了，代码的可读性越来越差、维护成本越来越高，甚至高过了重新开发的成本。



这个时候该怎么办呢？如果让你出出主意，你有什么好的建议吗？我们可以把==公共的功能、代码抽离出来==，形成一个独立的项目，部署成一个公共服务平台。所有金融产品的后端还是参照 MVC 三层架构独立开发，不过，它们只实现自己特有的功能，对于一些公共的功能，通过远程调用公共服务平台提供的接口来实现。

## 需求背景：

对于公共服务平台来说，接口请求==来自很多不同的系统==（后面统称为调用方），比如各种金融产品的后端系统。在系统上线一段时间里，我们遇到了很多问题。比如，==因为调用方代码 bug 、不正确地使用服务（比如启动 Job 来调用接口获取数据）、业务上面的突发流量（比如促销活动），导致来自某个调用方的接口请求数突增，过度争用服务的线程资源==，而来自其他调用方的接口请求，因此来不及响应而排队等待，导致接口请求的响应时间大幅增加，甚至出现超时。

为了解决这个问题，你有什么好的建议呢？我先来说说我的。我们可以开发==接口限流功能==，限制每个调用方对接口请求的频率。当超过预先设定的访问频率后，我们就触发限流熔断，比如，限制调用方 app-1 对公共服务平台总的接口请求频率不超过 1000 次 / 秒，超过之后的接口请求都会被拒绝。==除此之外，为了更加精细化地限流==，除了限制每个调用方对公共服务平台总的接口请求频率之外，我们还希望能对==单独某个接口的访问频率进行限制==，比如，限制 app-1 对接口 /user/query 的访问频率为每秒钟不超过 100 次。我们希望开发出来的东西有一定的影响力，即便做不到在行业内有影响力，起码也要做到在公司范围内有影响力。所以，从一开始，我们就不想把这个限流功能，做成只有我们项目可用。我们希望把它==开发成一个通用的框架==，能够应用到各个业务系统中，甚至可以集成到微服务治理平台中。实际上，这也体现了业务开发中要具备的抽象意识、框架意识。我们要善于识别出通用的功能模块，将它抽象成通用的框架、组件、类库等。

### 过程总结

- 调用方系统多：公共服务平台，接口请求来自不同的系统。
- 调用方不规范：bug、不正确的扩展、突发流量
- 解决方案分析：大体的流程分析，限制访问频率总体和单接口

## 需求分析:

前面我们已经讲过一些需求分析的方法，比如==画线框图、写用户用例、测试驱动开发==等等。这里，我们借助==用户用例和测试驱动开发==的思想，先去思考，==如果框架最终被开发出来之后，它会如何被使用==。我一般会找一个框架的应用场景，针对这个场景写一个框架使用的 Demo 程序，这样能够很直观地看到框架长什么样子。知道了框架应该长什么样，就相当于应试教育中确定了考试题目。针对明确的考题去想解决方案，这是我们多年应试教育锻炼之后最擅长做的。对于限流框架来说，我们来看下它的应用场景。首先我们需要设置限流规则。为了做到在不修改代码的前提下修改规则，我们一般会把规则放到==配置文件==中（比如 XML、YAML 配置文件）。在集成了限流框架的应用启动的时候，限流框架会将限流规则，按照事先定义的语法，解析并加载到内存中。我写了一个限流规则的 Demo 配置，如下所示：



``` properties

configs:
- appId: app-1
  limits:
  - api: /v1/user
    limit: 100
  - api: /v1/order
    limit: 50
- appId: app-2
  limits:
  - api: /v1/user
    limit: 50
  - api: /v1/order
    limit: 50
```

==在接收到接口请求之后，应用会将请求发送给限流框架，限流框架会告诉应用，这个接口请求是允许继续处理，还是触发限流熔断==。如果我们用代码来将这个过程表示出来的话，就是下面这个 Demo 的样子。如果项目使用的是 Spring 框架，我们可以利用 ==Spring AOP==，把这段限流代码放在统一的切面中，在切面中拦截接口请求，解析出请求对应的调用方 APP ID 和 URL，然后验证是否对此调用方的这个接口请求进行限流。

``` java

String appId = "app-1"; // 调用方APP-ID
String url = "http://www.eudemon.com/v1/user/12345";// 请求url
RateLimiter ratelimiter = new RateLimiter();
boolean passed = ratelimiter.limit(appId, url);
if (passed) {
  // 放行接口请求，继续后续的处理。
} else {
  // 接口请求被限流。
}
```

结合刚刚的 Demo，从使用的角度来说，限流框架主要包含两部分功能：==配置限流规则和提供编程接口==（RateLimiter 类）验证请求是否被限流。不过，作为通用的框架，除了==功能性需求==之外，==非功能性需求==也非常重要，有时候会决定一个框架的成败，比如，框架的易用性、扩展性、灵活性、性能、容错性等。

对于限流框架，我们来看它都有哪些非功能性需求。

==易用性方面==，我们希望限流规则的配置、编程接口的使用都很简单。我们希望提供==各种不同的限流算法==，比如基于内存的单机限流算法、基于 Redis 的分布式限流算法，能够让使用者自由选择。除此之外，因为大部分项目都是基于 Spring 开发的，我们还希望限流框架能非常方便地==集成到使用 Spring 框架==的项目中。

==扩展性、灵活性方面==，我们希望能够灵活地==扩展各种限流算法==。同时，我们还希望支持==不同格式==（JSON、YAML、XML 等格式）、==不同数据源==（本地文件配置或 Zookeeper 集中配置等）的限流规则的配置方式。性能方面，因为每个接口请求都要被检查是否限流，这或多或少会增加接口请求的响应时间。而对于响应时间比较敏感的接口服务来说，我们要让==限流框架尽可能低延迟==，尽可能减少对接口请求本身响应时间的影响。

==容错性方面==，接入限流框架是为了==提高系统的可用性、稳定性==，不能因为==限流框架的异常，反过来影响到服务本身的可用性==。所以，限流框架要有高度的容错性。比如，分布式限流算法依赖集中存储器 Redis。如果 Redis 挂掉了，限流逻辑无法正常运行，这个时候业务接口也要能正常服务才行。



从今天的讲解中，不知道你有没有发现，基本的功能需求其实没有多少，但将==非功能性需求==考虑进去之后，明显就复杂了很多。还是那句老话，==写出能用的代码很简单，写出好用的代码很难==。对于限流框架来说，非功能性需求是设计与实现的难点。怎么做到易用、灵活、可扩展、低延迟、高容错，才是开发的重点，也是我们接下来两节课要讲解的重点。

### 过程总结（如何进行系统设计）

- 功能性需求分析--==产出业务流程图、用户用例、主要模块、MVP功能==
  - ==分析手段==：通过流程框图、用户用例、TDD等进行分析。
  - ==MVP==：预想框架的样子，以及用户如何使用，设定目标有针对性的思考。（配置文件，spring aop）
  - ==业务流程==：用户请求--限流框架--规则读取--判断限流？--正常请求（拒绝请求）产出流程图及具体流程
  - ==主要模块==：分析主要模块：提供限流规则配置和编程接口

- 非功能性需求分析：==易用、灵活、可扩展、低延时、高容错==
  - ==易用性==：规则配置简单、接口使用简单、提供多种限流算法、与spring集成方便
  - ==扩展性、灵活性==：扩展算法、支持不同的格式 、不同的数据源、低延时
  - ==容错性==：高度容错，不能影响接口本身的性能。



## 系统设计

前面提到，我们把项目实战分为==分析、设计、实现三部分==来讲解。其中，分析环节跟之前讲过的面向对象分析很相似，都是做需求的梳理。但是，项目实战中的设计和实现，跟面向对象设计和实现就不是一回事儿了。这里的==“设计”指的是系统设计，主要是划分模块，对模块进行设计。==这里的==“实现”实际上等于面向对象设计加实现。因为我们前面讲到，面向对象设计与实现是聚焦在代码层面的，主要产出的是类的设计和实现。==今天，我们分限==流规则、限流算法、限流模式、集成使用这 4 个模块==，来讲解限流框架的设计思路。上节课我们提到，限流框架的基本功能非常简单，复杂在于它的非功能性需求，所以，我们今天讲解的重点是，看如何通过合理的设计，实现一个满足易用、易扩展、灵活、低延时、高容错等非功能性需求的限流框架。

### 过程总结

- ==项目实战：需求分析、系统设计、系统实现三部分。==
- ==系统设计：划分模块，对模块进行设计。==
- ==系统实现：面向对象的设计加实现，聚集代码层面，产出类的设计与实现==

### 具体模块

#### 限流规则

框架需要==定义限流规则的语法格式==，包括==调用方、接口、限流阈值、时间粒度这几个元素==。框架用户按照这个语法格式来==配置限流规则==。我举了一个例子来说明一下，如下所示。其中，unit 表示限流时间粒度，默认情况下是 1 秒。limit 表示在 unit 时间粒度内最大允许的请求次数。拿第一条规则来举例，==它表示的意思就是：调用方 app-1 对接口 /v1/user 每分钟的最大请求次数不能超过 100 次。==

``` properties

configs:
- appId: app-1
  limits:
  - api: /v1/user
    limit: 100
    unit：60
  - api: /v1/order
    limit: 50
- appId: app-2
  limits:
  - api: /v1/user
    limit: 50
  - api: /v1/order
    limit: 50
```

对于==限流时间粒度==的选择，我们既可以选择限制 1 秒钟内不超过 1000 次，也可以选择限制 10 毫秒内不超过 10 次，还可以选择限制 1 分钟内不超过 6 万次。虽然看起来这几种限流规则是等价的，但过大的时间粒度会达不到限流的效果。比如，有可能 6 万次请求集中在 1 秒中到达，限制 1 分钟不超过 6 万次，就起不到保护的作用；相反，因为接口访问在细时间粒度上随机性很大，并不会很均匀。过小的时间粒度，会误杀很多本不应该限流的请求。所以，尽管越细的时间粒度限流整形效果越好，流量曲线越平滑，但也并不是时间粒度越小越合适。我们知道，Spring 框架支持==各种格式的配置文件==，比如 XML、YAML、Porperties 等。除此之外，基于==约定优于配置原则==，Spring 框架用户只需要将配置文件按照约定来命名，并且放置到约定的路径下，Spring 框架就能按照约定自动查找和加载配置文件。大部分 Java 程序员已经习惯了 Spring 的配置方式，基于我们前面讲的最小惊奇原则，在限流框架中，我们也==延续 Spring 的配置方式==，支持 XML、YAML、Properties 等几种配置文件格式，同时，约定默认的配置文件名为 ==ratelimiter-rule.yaml==，默认放置在 classpath 路径中。除此之外，为了提高框架的兼容性、易用性，除了刚刚讲的本地文件的配置方式之外，我们还希望兼容从==其他数据源==获取配置的方式，比如 Zookeeper 或者自研的配置中心。

##### 过程总结

- ==具体流程和异常分析==：限流时间粒度分析，不同的限流规则有不同的效果。要适合业务需求。
- ==配置文件支持方案==：结合支持多种格式，并且约定优于配置，与spring结合 。
- ==其他数据源的扩展==：zk，自研配置中心等

#### 限流算法

常见的限流算法有：==固定时间窗口限流算法、滑动时间窗口限流算法、令牌桶限流算法、漏桶限流算法==。其中，固定时间窗口限流算法最简单。我们只需要选定一个起始时间起点，之后每来一个接口请求，我们都给计数器（记录当前时间窗口内的访问次数）加一，如果在当前时间窗口内，根据限流规则（比如每秒钟最大允许 100 次接口请求），累加访问次数超过限流值（比如 100 次），就触发限流熔断，拒绝接口请求。当进入下一个时间窗口之后，计数器清零重新计数。

不过，固定时间窗口的限流算法的缺点也很明显。这种算法的限流策略过于粗略，无法应对两个时间窗口临界时间内的突发流量。我们来举一个例子。假设我们限流规则为每秒钟不超过 100 次接口请求。第一个 1 秒时间窗口内，100 次接口请求都集中在最后的 10 毫秒内，在第二个 1 秒时间窗口内，100 次接口请求都集中在最开始的 10 毫秒内。虽然两个时间窗口内流量都符合限流要求 (小于等于 100 个接口请求)，但在两个时间窗口临界的 20 毫秒内集中有 200 次接口请求，固定时间窗口限流算法没法对这种情况进行限流，集中在这 20 毫秒内的 200 次请求有可能会压垮系统。

为了让流量更加平滑，==于是就有了更加高级的滑动时间窗口限流算法、令牌桶限流算法和漏桶限流算法==。因为我们主要讲设计而非技术，所以其他几种限流算法，留给你自己去研究，你也可以参看我之前写的关于限流框架的技术文档。尽管固定时间窗口限流算法没法做到让流量很平滑，但大部分情况下，它已经够用了。默认情况下，框架使用固定时间窗口限流算法做限流。不过，考虑到框架的扩展性，我们需要预先做好设计，预留好扩展点，方便今后扩展其他限流算法。除此之外，为了提高框架的易用性、灵活性，我们最好将其他几种常用的限流算法，也在框架中实现出来，供框架用户根据自己业务场景自由选择。

##### 过程总结

- ==核心算法对比==：根据业务场景对比核心限流算法，找出适合场景的方案

#### 限流模式

刚刚讲的是限流算法，我们再讲讲限流模式。我们把==限流模式分为两种：单机限流和分布式限流。==所谓单机限流，就是针对单个实例的访问频率进行限制。注意这里的单机并不是真的一台物理机器，而是一个服务实例，因为有可能一台物理机器部署多个实例。所谓的分布式限流，就是针对某个服务的多个实例的总的访问频率进行限制。我举个例子来解释一下。

假设我们==开发了一个用户相关的微服务==，为了提高服务能力，我们部署了 5 个实例。我们限制某个调用方，对单个实例的某个接口的访问频率，不能超过 100 次 / 秒。这就是单机限流。我们限制某个调用方，对 5 个实例的某个接口的总访问频率，不能超过 500 次 / 秒。这就是所谓的分布式限流。

从实现的角度来分析，单机限流和分布式限流的主要区别在接口访问计数器的实现。单机限流只需要在单个实例中维护自己的接口请求计数器。而分布式限流需要==集中管理计数器==（比如使用 Redis 存储接口访问计数），这样才能做到多个实例对同一个计数器累加计数，以便实现对多个实例总访问频率的限制。

前面我们讲到框架要==高容错，不能因为框架的异常，影响到集成框架的应用的可用性和稳定性==。除此之外，我们还讲到框架要低延迟。限流逻辑的执行不能占用太长时间，不能或者很少影响接口请求本身的响应时间。因为分布式限流基于外部存储 Redis，网络通信成本较高，实际上，高容错、低延迟设计的主要场景就是基于 Redis 实现的分布式限流。

对于 Redis 的各种异常情况，我们处理起来并不难，==捕获并封装为统一的异常==，向上抛出或者吞掉就可以了。比较难处理的是 Redis 访问超时。Redis 访问超时会严重影响接口的响应时间，甚至导致接口请求超时。所以，在访问 Redis 时，我们需要==设置合理的超时时==间。一旦超时，我们就判定为限流失效，继续执行接口请求。Redis 访问超时时间的设置既不能太大也不能太小，太大可能会影响到接口的响应时间，太小可能会导致太多的限流失效。我们可以通过压测或者线上监控，获取到 Redis 访问时间分布情况，再结合接口可以容忍的限流延迟时间，权衡设置一个较合理的 Redis 超时时间。

##### 过程总结

- ==模式区别==：对比单机限流 和分布式限流 的区别，以为对应的实现方案
- ==容错处理==：框架异常不能影响服务本身的能力
- ==性能考虑==：依赖于redis，设置合理的超时时间

#### 限流集成

集成使用前面剖析 Spring 框架的时候，我们讲到==低侵入松耦合设计思想==。限流框架也应该满足这个设计思想。因为框架是需要集成到应用中使用的，我们希望==框架尽可能低侵入，与业务代码松耦合，替换、删除起来也更容易些。==除此之外，在剖析 MyBatis 框架的时候，我们讲到 MyBatis 框架是为了简化数据库编程。实际上，为了进一步简化开发，==MyBatis 还提供了 MyBatis-Spring 类库==，方便在使用了 Spring 框架的项目中集成 MyBatis 框架。我们也可以借鉴 MyBatis-Spring，开发一个 Ratelimiter-Spring 类库，能够方便使用了 Spring 的项目集成限流框架，将易用性做到极致。

##### 过程总结

- ==遵守设计原则==：低侵入、松耦合、参考mybaits-spring类库，方便的与spring进行集成，易用到极致。



## ratelimiter实现

https://blog.csdn.net/netyeaxi/article/details/104270337



## 系统实现 //todo

experience-notes  有道 



# 案例二：幂等框架设计



# 案例三：灰度发布设计





# 案例四：安全队列的实现

> blockingQueue   ArrayBlockingQueue  LinkedBlockingQueue
>
> 实现步骤：1，2，3





# 案例五：对外接口如何设计





# 案例六：pv、uv统计框架

- 参考：

  https://www.zhihu.com/question/47001965/answer/2469455497

  https://zhuanlan.zhihu.com/p/379789172

  spark-streaming    https://zhuanlan.zhihu.com/p/87129375

  通过apache日志分析

  https://www.zhihu.com/question/20128538/answer/51895004

  pv、uv概念

  https://www.zhihu.com/question/21556347/answer/2253564711

  实现pv uv统计_如何优雅快速的统计千万级别uv？

  https://blog.csdn.net/weixin_29732787/article/details/112700251



- 基于redis实现uv

Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。

在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。

比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。



# 案例七：优惠券平台

> https://www.toutiao.com/article/7087825428798652962/?log_from=7d1a8006fc296_1656550576733
>
> https://www.toutiao.com/article/7114228506829488640/



- 难点一：优惠券模板查询

  - 查询优惠券详情时，流程是先查询redis缓存。缓存中会缓存所有的优惠券模板数据，如果查询不到，再查询mysql数据库。
  
  - 高峰期的查询QPS会达到10w，对存储会造成比较大的压力 。
  
  - 根据压测mysql单节点支持的写入极限是  4000 QPS，超过这个极限，延时会比较严重。
  
  - redis单片写入支持2w 左右，读取支持10w左右
  
  - 解决方案：一是读写分离，在查询券模板和查询券记录，对mysql进行主从读写分离处理，从而可以减轻主库的压力。
  
    
  
  

# 案例八：分布事务案例

2pc强一致性实现：   订单和优惠券

消息型事务：订单和购物车

https://blog.csdn.net/zhizhengguan/article/details/121344315

分布式事务解决方案

https://blog.csdn.net/inrgihc/article/details/108673899



https://blog.csdn.net/qq_31960623/article/details/123112229



https://www.cnblogs.com/lanpo/articles/12824349.html



![image-20220705151203666](https://tva1.sinaimg.cn/large/e6c9d24ely1h3w2k6ewbmj20fk0bkaad.jpg)

seata

极客时间 

应用场景：

下订单、支付 、减库存

CR--消息  日志

https://www.jianshu.com/p/cdc4510fab17



# 案例九：秒杀

 参考：

交易系统https://blog.csdn.net/uhana/article/details/114296909https://blog.csdn.net/itomge/article/details/104338751?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_title~default-0.pc_relevant_default&spm=1001.2101.3001.4242.1&utm_relevant_index=2***

https://blog.csdn.net/ybygjy/article/details/50443526 

# 案例十：文件系统

> Coupon内存文件系统设计案例   

使用DDD设计文件系统时，可以遵循以下主要步骤和思路：
- 领域划分：
根据业务需求和领域知识，识别出核心的业务领域，如：存储管理、元数据管理、用户权限管理、目录结构管理等。
- 定义领域模型：
对每个领域进行深入分析，定义其关键实体（Entities）和值对象（Value Objects）。例如，在存储管理域中，File可能是一个实体，具有ID、路径、大小等属性；在用户权限管理域中，User、Permission可能是实体或值对象。
设计聚合根（Aggregate Roots），例如，Directory作为目录结构管理的聚合根，包含子文件和子目录；FileStorage作为存储管理的聚合根，负责文件的实际读写操作。
- 边界上下文（Bounded Contexts）：
明确各个领域的边界，比如“存储服务上下文”、“元数据管理上下文”、“访问控制上下文”。不同上下文之间通过接口交互，保持上下文之间的独立性。
- 领域服务（Domain Services）：
定义领域服务来处理复杂的业务逻辑，如用户对文件的操作权限验证（AccessControlService）、目录结构的维护操作（DirectoryService）等。
- 仓储（Repositories）：
为每个聚合根设计对应的仓储接口，用于持久化领域模型。例如，FileRepository用于持久化文件相关的元数据信息，DirectoryRepository用于持久化目录结构信息。
- 应用服务（Application Services）：
构建应用服务层，封装领域服务并对外提供API接口，供客户端调用。例如，创建文件、删除文件、更改文件权限等功能可通过应用服务实现。
- 领域事件（Domain Events）：
在领域模型中引入事件驱动的设计，当发生重要业务事件时发布领域事件，例如文件被修改后触发FileUpdatedEvent，由事件处理器进行后续处理，如更新缓存、通知其他服务等。
集成与反向工程：
将以上设计落地到代码层面，编写实体、值对象、领域服务、仓库以及应用服务的实现，并确保它们符合DDD原则，如保证事务一致性、遵循统一语言Ubiquitous Language等。
通过上述步骤，您可以逐步构建一个基于DDD理念的文件系统，它不仅能够有效组织代码结构，还能更好地应对复杂业务场景和未来变化的需求。

# 案例十一：短链接

https://mp.weixin.qq.com/s?__biz=MzIzNjY1OTk5OQ==&mid=2247484733&idx=1&sn=85a7d33e782cba9ffd17121203cbb306&chksm=e8d5314ddfa2b85bc3f75f7a045f3ff2e3213154eef66fc56af74836a314bd2f8b0acc07bbe3&cur_album_id=2227238756665950210&scene=189#wechat_redirect



# 分布式链路追踪系统设计

设计一个分布式链路追踪系统的核心技术主要包括以下几个方面：

1. 全局唯一标识符（Trace ID / Span ID / Parent Span ID）：
Trace ID：标识一次完整的分布式请求链路，贯穿整个请求生命周期。
Span ID：标识一个独立的工作单元，如一次HTTP请求、数据库查询或远程服务调用。
Parent Span ID：用于表示父子关系，一个Span的Parent Span ID指向它的上一级调用的Span ID。
2. 追踪数据模型与协议：
使用统一的数据模型来标准化追踪信息，包括时间戳、操作名、开始和结束时间、耗时、状态、标记(tags)和日志(logs)等。
支持开放标准，如OpenTracing或OpenTelemetry，这些标准定义了跨语言和平台的追踪数据接口和传输格式。
3. 追踪数据注入与传播：
客户端SDK：在各个服务节点中注入和收集追踪数据，通过拦截器、过滤器等方式自动嵌入到请求和响应中。
4. 跨进程传播：在微服务之间通过HTTP Header（如X-B3-TraceId、X-B3-SpanId等）或其他方式传递Trace和Span信息。
5. 追踪数据收集与存储：
收集器：接收来自各个服务节点的追踪数据，通过gRPC、HTTP、消息队列等方式传输。
存储系统：设计适合大规模链路数据存储的解决方案，可以选用关系型数据库（如MySQL）、时序数据库（如InfluxDB）、NoSQL数据库（如Cassandra）或者专门的追踪数据存储系统（如Elasticsearch）。
6. 分布式追踪数据压缩与归档：
使用数据压缩算法减少传输和存储成本，例如对连续的整数序列进行差分编码，或对重复的数据结构进行哈夫曼编码。
设计合理的数据保留策略，如只保留最近一段时间内的原始数据，较早的数据进行归档或删除。
6. 数据索引与查询优化：
为提高查询效率，建立合适的数据索引，如基于Trace ID、时间戳、服务名等维度的索引。
实现高效的查询引擎，快速定位和聚合相关链路信息。
7. 可视化与分析工具：
建立强大的可视化界面，展现分布式请求的完整调用链路拓扑图和时间轴。
提供多种查询和过滤功能，帮助用户发现性能瓶颈、定位故障原因。
8. 性能监控与告警：
结合追踪数据进行实时性能监控，如响应时间、成功率等关键指标。
设定阈值触发告警，及时发现潜在的问题。
9. 扩展性与容错：
确保追踪系统的水平扩展能力，可以根据负载动态增加收集器和服务端资源。
实现容错和数据冗余，防止单点故障导致数据丢失。
综上所述，设计分布式链路追踪系统不仅需要考虑数据的产生、收集、存储和查询，还要注重系统的性能、扩展性和用户体验，从而形成一套完整的全链路可观测解决方案。

实现方案：SpringCloud+Zipkin+Sleuth(参见 实践设计部分)



# IM（internet message）即时通信系统	

### 设计一个消息待发系统

要求：1000w消息待发送，如何设计

问题：





### IM系统学习

>  https://juejin.cn/post/6844903935040307214

![image-20221014105648233](https://tva1.sinaimg.cn/large/008vxvgGly1h74mrlmk3pj30u00w8mzv.jpg)

- 如何保证消息的顺序和唯一
- 多个设备在线如何保证消息一致性
- 如何处理消息发送失败
- 消息的安全性
- 如果要存储聊天记录要怎么做
- 数据库分表分库
- 服务高可用

![image-20221014113708209](/Users/haibingzhang/d/weiyundata/typoradoc/images/image-20221014113708209.png)



![image-20221014113732993](/Users/haibingzhang/d/weiyundata/typoradoc/images/image-20221014113732993.png)

https://github.com/crossoverJie/cim

- `CIM` 中的各个组件均采用 `SpringBoot` 构建。
- 采用 `Netty` 构建底层通信。
- `Redis` 存放各个客户端的路由信息、账号信息、在线状态等。
- `Zookeeper` 用于 `IM-server` 服务的注册与发现。

### cim-server

`IM` 服务端；用于接收 `client` 连接、消息透传、消息推送等功能。

**支持集群部署。**

### cim-forward-route

消息路由服务器；用于处理消息路由、消息转发、用户登录、用户下线以及一些运营工具（获取在线用户数等）。

### cim-client

`IM` 客户端；给用户使用的消息终端，一个命令即可启动并向其他人发起通讯（群聊、私聊）。

##



# 项目经历（star）

## 京东商城



促销系统 ： http://www.woshipm.com/pd/3345169.html

==订单和交易== ： https://blog.csdn.net/weixin_42586723/article/details/119651204  包含核心代码 

``` text
订单提交
幂等性处理(token令牌机制)
准备好订单确认数据后，返回给用户看运费等信息，同时创建防重令牌redis.set(‘order:token:(userId)’,uuid)，一并返回；

用户点击提交订单按钮，带着token(hidden元素带着)；

渲染订单确认页，后台处理的时候确认请求带过来token的uuid和redis库中是否一致；

此处是重点，比对后立刻删除，比对和删除要求具有原子性，通过redis-lua脚本完成；

提交订单时不要提交购买的商品，去购物车数据库重新获取即可，防止购物车变化和修改页面值；

但可以提交总额，防止商品金额变了还提交订单，用户不满意；

```



京东后台：订单履约系统设计上和下

https://blog.csdn.net/itfly8/article/details/118425863

https://blog.csdn.net/itfly8/article/details/118425928?spm=1001.2101.3001.6650.1&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-118425928-blog-118425863.pc_relevant_aa&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-118425928-blog-118425863.pc_relevant_aa&utm_relevant_index=2

ocs：order   OFC：order fullfill center

京东订单体系：https://www.jianshu.com/p/b352d52457ba

京东时效计算：xx

京东促销模块



咚咚架构： https://blog.csdn.net/lsxf_xin/article/details/80063252

京东商城架构设计原则：

https://blog.csdn.net/niulu90/article/details/116701306?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-116701306-blog-105062166.pc_relevant_antiscanv4&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1-116701306-blog-105062166.pc_relevant_antiscanv4&utm_relevant_index=1

京东商城，超大型电商系统架构设计原则与实践！8页ppt详解

https://blog.csdn.net/yellowzf3/article/details/105062166



# DevOps云原生系统

设计一个DevOps云原生系统，并与Docker和Kubernetes（k8s）整合，需要从领域驱动设计（DDD, Domain-Driven Design）、功能需求、非功能性需求以及实现方案四个维度进行分析。
## DDD分析：
- 核心域：
容器编排域（Container Orchestration Domain）：此域负责管理和调度Docker容器在Kubernetes集群中的生命周期。包括服务部署、滚动更新、水平扩展、健康检查等操作。
CI/CD流程域（Continuous Integration and Continuous Deployment Domain）：关注构建、测试、打包、发布等自动化流程，确保应用能够在DevOps流水线上快速迭代并稳定地部署到生产环境。
- 通用子域：
资源管理子域（Resource Management Subdomain）：处理存储、网络、计算等基础设施资源的分配和监控。
安全控制子域（Security Control Subdomain）：提供身份验证、授权、加密等安全保障措施，确保容器和服务的安全运行。
- 支撑子域：
日志审计子域（Logging and Auditing Subdomain）：收集并聚合容器和服务产生的日志信息，便于运维人员进行故障排查和合规审计。
监控报警子域（Monitoring and Alerting Subdomain）：实时监控集群及应用状态，设置阈值触发告警，确保及时发现和解决问题。
## 功能性需求：
容器镜像管理：支持通过Git或其他源代码仓库集成，自动构建Docker镜像。
Kubernetes资源配置：自动生成或更新yaml配置文件，定义Deployment、Service、ConfigMap等资源对象。
CI/CD流水线：支持多阶段的持续集成和持续部署过程，如单元测试、静态代码扫描、构建、推送镜像、部署到不同环境。
蓝绿部署、金丝雀发布：支持多种部署策略以减少应用更新对用户的影响。
资源伸缩：根据预设条件或手动指令动态调整Pod数量以满足负载变化的需求。
## 非功能性需求：
高可用性：系统应具有自我修复能力，当节点或服务出现故障时能够快速恢复。
安全性：支持RBAC权限控制，保证资源访问的安全；支持TLS加密传输，保护数据隐私。
可扩展性：系统架构设计上易于扩展，能随着业务增长而平滑扩容。
可观测性：具备全面的监控指标，支持日志查询、性能跟踪等功能。
兼容性：兼容多种编程语言和技术栈的应用部署，支持多种第三方工具和插件的接入。
## 实现方案：
使用Jenkins、GitLab CI/CD或者阿里云CodePipeline等工具搭建CI/CD流水线。
通过Helm模板或者其他工具生成Kubernetes资源描述文件。
利用Kubernetes API对接底层资源，实现容器的自动化部署和管理。
引入Prometheus、Grafana、ELK stack等工具组合实现监控、日志管理和报警功能。
结合Istio、Linkerd等服务网格技术增强服务间的通信治理和安全控制。
采用Travis CI、GitHub Actions等云服务商提供的CI/CD服务可以简化开发和维护工作。
最后，在整个系统的建设过程中，需遵循云原生的设计原则，比如微服务化、无服务器化、声明式API管理、标准化的容器镜像格式等，以充分发挥Docker和Kubernetes的优势。同时，要充分考虑企业级DevOps场景下的复杂性和个性化需求，灵活定制解决方案。



# 监控系统

设计一个基于DDD的监控系统的采集模块，我们需要定义领域模型、识别核心实体和聚合，并结合功能性需求与非功能性需求来设计。

- DDD设计思路：

  领域模型：
  MetricAggregate：作为采集模块的核心聚合根，代表一个具体的监控指标（如CPU使用率、内存占用等）。
  MetricType：枚举或值对象，表示不同的监控类型。
  MetricDataPoint：实体，包含度量的时间戳和具体数值。
  CollectorService：应用服务，负责接收外部系统发送的监控数据并更新MetricAggregate。
  边界上下文：
  MetricsCollectionContext：专注于处理监控数据的采集、存储和分析。
  功能性需求：
  数据上报接口：提供API给外部系统上报实时监控数据。
  批量处理：支持高并发场景下高效地接收大量数据点。
  持久化：将接收到的数据点存储到数据库或时序数据库中。
  非功能性需求：
  高性能：支持QPS在10w左右的数据采集能力。
  稳定性：确保在高负载下系统的稳定性和健壮性，避免数据丢失。
  可扩展性：设计模块以易于水平扩展，随着业务增长可以增加更多实例。



## 111

设计一个基于DDD的监控系统的采集模块，我们需要定义领域模型、识别核心实体和聚合，并结合功能性需求与非功能性需求来设计。
DDD设计思路：
领域模型：
MetricAggregate：作为采集模块的核心聚合根，代表一个具体的监控指标（如CPU使用率、内存占用等）。
MetricType：枚举或值对象，表示不同的监控类型。
MetricDataPoint：实体，包含度量的时间戳和具体数值。
CollectorService：应用服务，负责接收外部系统发送的监控数据并更新MetricAggregate。
边界上下文：
MetricsCollectionContext：专注于处理监控数据的采集、存储和分析。
功能性需求：
数据上报接口：提供API给外部系统上报实时监控数据。
批量处理：支持高并发场景下高效地接收大量数据点。
持久化：将接收到的数据点存储到数据库或时序数据库中。
非功能性需求：
高性能：支持QPS在10w左右的数据采集能力。
稳定性：确保在高负载下系统的稳定性和健壮性，避免数据丢失。
可扩展性：设计模块以易于水平扩展，随着业务增长可以增加更多实例

## api

对外提供的API可能包括但不限于以下接口：
数据上报接口：
POST /api/metrics：接收应用或服务发送的监控指标数据，如CPU使用率、内存占用、请求响应时间等。
POST /api/events：上报事件型日志，例如错误、警告信息。
告警规则管理接口：
POST /api/alarm-rules：创建或更新告警规则。
GET /api/alarm-rules：获取所有告警规则列表。
DELETE /api/alarm-rules/{id}：删除指定ID的告警规则。
实时监控数据查询接口：
GET /api/metrics/timeseries?metricType=cpu_usage&startTimestamp=<timestamp>&endTimestamp=<timestamp>：按时间序列获取指定类型的监控数据。
告警状态查询接口：
GET /api/alarms?status=open&since=<timestamp>：查询指定时间内未处理的告警事件。
通知设置接口：
POST /api/notifications/settings：设置告警通知方式（邮件、短信、电话）。
对于QPS在10w左右的采集模块设计，需要考虑以下关键点：
高并发数据上报：
使用无状态的设计，确保每个上报请求都能快速处理并释放资源。
优化网络层，如采用HTTP/2、gRPC等协议减少连接开销。
针对上报数据量大和频率高的场景，可使用异步处理机制，并且将数据暂存到消息队列（如RabbitMQ、Kafka）中，避免直接写入数据库导致瓶颈。
如果是微服务架构，可以考虑服务网格技术（如Istio）进行流量管理和负载均衡。
后端处理逻辑：
数据入库环节，采用批量插入或者流式处理方式，减少数据库操作次数。
对于实时分析需求，可以利用InfluxDB、Prometheus等时序数据库来存储和快速检索监控数据。
水平扩展：
根据业务需求，在集群环境中部署多个API服务实例，通过负载均衡器分发请求。
采集模块应具备良好的水平扩展能力，可根据实际流量动态增减节点。
缓存与降级策略：
对于部分读多写少的数据，可以采用缓存系统（如Redis）来减轻数据库压力。
设计合理的降级策略，在高流量冲击下保证核心功能正常运行。
监控与报警：
自身系统的健康状况也要纳入监控范围，确保采集模块本身的稳定性。
当系统性能接近阈值时，触发自定义报警机制，以便及时调整资源分配

## 实现代码 

``` java
// MetricType枚举
public enum MetricType {
    CPU_USAGE,
    MEMORY_USAGE,
    REQUEST_COUNT,
    // 其他监控类型...
}

// MetricDataPoint实体
public class MetricDataPoint {
    private UUID id;
    private MetricType type;
    private double value;
    private LocalDateTime timestamp;

    // 省略getter和setter方法
}

// MetricAggregate聚合根
@AggregateRoot
public class MetricAggregate {
    @Id
    private UUID id;
    private String name; // 指标名称
    private List<MetricDataPoint> dataPoints = new ArrayList<>();

    public void addDataPoint(MetricDataPoint dataPoint) {
        this.dataPoints.add(dataPoint);
        // 在这里可以添加事件监听器，当数据点添加后触发持久化操作
    }

    // 省略其他业务逻辑和getter、setter方法
}

// CollectorService应用服务
@Service
public class CollectorService {
    @Autowired
    private MetricRepository metricRepository;
    
    @Autowired
    private EventPublisher eventPublisher;

    public void collectMetricData(MetricDataPoint dataPoint) {
        // 根据dataPoint的metricType找到对应的MetricAggregate
        MetricAggregate metric = metricRepository.findByIdAndType(dataPoint.getId(), dataPoint.getType());
        
        if (metric == null) {
            // 创建新的MetricAggregate
            metric = new MetricAggregate();
            metric.setId(dataPoint.getId());
            // 初始化其他属性...
        }
        
        metric.addDataPoint(dataPoint);
        
        // 通过Event Sourcing模式触发保存事件
        eventPublisher.publish(new MetricDataCollectedEvent(metric, dataPoint));
    }
}

// MetricRepository接口
public interface MetricRepository extends JpaRepository<MetricAggregate, UUID>, JpaSpecificationExecutor<MetricAggregate> {
    // 可根据需求自定义查询方法
}

```



## 代码扫描

>  扩展sonar插件：   https://blog.csdn.net/wmq880204/article/details/117866872

ast解析框架：   https://github.com/LaplaceDemon/light-expr



## 代码评审

gitlab高可用如何实现？

Gitlab  sidequeue的问题？

gitlab如何实现主备？




# crokking the system design

> https://download.csdn.net/download/judyintech/13837711?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-download-2%7Edefault%7ECTRLIST%7EPaid-1-13837711-blog-106576210.pc_relevant_default&depth_1-utm_source=distribute.pc_relevant_t0.none-task-download-2%7Edefault%7ECTRLIST%7EPaid-1-13837711-blog-106576210.pc_relevant_default&utm_relevant_index=1
>
> https://blog.csdn.net/NXHYD/article/details/122612092
>
> 设计一个 TinyURL服务
>
> https://mp.weixin.qq.com/s?__biz=MzIzNjY1OTk5OQ==&mid=2247484733&idx=1&sn=85a7d33e782cba9ffd17121203cbb306&chksm=e8d5314ddfa2b85bc3f75f7a045f3ff2e3213154eef66fc56af74836a314bd2f8b0acc07bbe3&cur_album_id=2227238756665950210&scene=189#wechat_redirect

标准流程：

1. 需求澄清：解决的问题是什么,产出流程图及需求功能和非需求功能（领域划分）
1. 系统接口定义API：希望这个系统能做什么，产出接口文档。
1. 粗略估算: 估算一下规模（现有或未来），产出容量、性能等要求
1. 定义数据模型：理顺数据在模块中的流动，分区和管理，产出E-R图（领域模型）
1. 高级定义: 识别出端到端的各个模块，产出核心模块和类图 
1. 细节定义：对模块进行分析，产出不同的方案并进行tradeoff.
1. 解决瓶颈：找到风险，产出预案（非功能性设计）

# tiwwiter设计

## 分析

1. 需求澄清
   - 问题
     能发tiwwiter，关注别人？
     有时间线展示？
     只包含后端，还是有前端？
     显示图片、视频？
     能搜索？
     展示热点话题？
   - 功能性需求：
     用户可以注册和登录。
     用户可以发布 tweet。
     用户可以查看其他用户发布的 tweet。
     用户可以关注其他用户。
     系统可以根据用户的兴趣和行为历史推荐相关的 tweet。
   - 非功能性需求：
     系统需要保证用户数据的安全性和隐私保护。
     系统需要具备高可用性和容错性，以应对大量用户同时访问。
     系统需要具备可扩展性，以应对数据量的增长。
     系统需要提供友好的用户界面和响应快速的用户体验。


2. 系统接口定义
- postTweet(user_id, tweet_data, user_location, tweet_location,timeline...)
- generateTimeline(user_id, curr_time,..)
- followPeople(user_id, followee_id,..)
- markFavorateTweet(user_id, tweet_id, timestamp...)

3. 粗略估算
- 系统预期规模多大？（新推特的数量，浏览量，每秒产生的时间线数量等）
- 系统需要多少内存？ 如果有图片和视频情况是否相同？
- 系统需要多大的带宽？ 处理网络拥挤和负载的关键

4. 定义数据模型
- user: userId, name, email,createDate,modifiedDate, 
- tweet: tweetId, content, numOfLikes, tweetLocation, createDate, modifiedDate, timestamp
- userFoll  ow: userId1, userId2
- favoriteTweet: userId, tweetId, timestamp

5. 高级定义
图像和视频存储、数据库存储

6. 细节定义
- 如何把大量的数据存储到不同的数据库？是否应该把一个用户的数据存储到一个数据库？影响？
- 如果处理活跃用户？如大量发和大量关注的人？
- 时间线显示为最近，如果优化最近的存储？
- 引入缓存加速，引入到什么程度，从哪个层面？
- 哪个模块需要做好负载？

7. 解决瓶颈
- 系统中可能失效的点，如何解决？
- 系统中对数据的备份有吗？如果数据库失效，对用户影响？
- 依赖服务出问题后，有没有兜底方案？
- 如果快速的发现问题，有没有监控？
- Twitter 系统设计的难点包括：
  高并发访问：由于 Twitter 是一个社交媒体平台，用户数量非常庞大，因此系统需要能够处理高并发访问的挑战。
  数据存储和检索：Twitter 系统需要存储大量的用户信息、推文数据、关注关系等，需要设计高效的数据库存储和检索方案。
  推荐算法：Twitter 系统需要根据用户的兴趣和行为历史推荐相关的推文，需要设计高效的推荐算法。
  安全性和隐私保护：Twitter 系统需要保证用户数据的安全性和隐私保护，防止数据泄露和滥用。
  可扩展性：随着用户数量的增长，Twitter 系统需要能够轻松扩展，以应对数据量和访问量的增长。
  用户界面和用户体验：Twitter 系统需要提供友好的用户界面和响应快速的用户体验，以吸引和留住用户。

## 实现代码

``` java
// 用户类
public class User {
    private String username;
    private String password;
    private String name;
    // 其他属性和方法
}

// Tweet 类
public class Tweet {
    private String content;
    private String mediaType; // 文本、图片、视频等
    private String mediaUrl; // 媒体资源的 URL
    private User author;
    // 其他属性和方法
}

// 关注关系类
public class Follow {
    private User follower;
    private User followee;
    // 其他属性和方法
}

// 推荐算法类
public class Recommendation {
    private List<User> users;
    private List<Tweet> tweets;
    // 其他属性和方法
}

// 数据存储类（使用内存实现，实际应用中需要使用数据库）
public class DataStore {
    private Map<String, User> users;
    private Map<String, Tweet> tweets;
    private Map<String, Follow> follows;
    // 其他属性和方法
}

// 用户注册和登录
public class AuthenticationService {
    private DataStore dataStore;
    // 其他属性和方法
}

// 发布 tweet
public class TweetService {
    private DataStore dataStore;
    // 其他属性和方法
}

// 关注其他用户
public class FollowService {
    private DataStore dataStore;
    // 其他属性和方法
}

// 推荐 tweet
public class RecommendationService {
    private DataStore dataStore;
    // 其他属性和方法
}

// 数据存储和检索
public class DataStorage {
    private DataStore dataStore;
    // 其他属性和方法
}

```




# 设计一个 TinyURL服务

1. 需要澄清：解决什么问题？产出流程图（领域划分）
- 对url创建比较短的别名，重定向到原始链接
- 用于优化跨设备连接，跟踪链接进行分析
- 功能性：
    - 对于url，生成一个短且唯一别名，短链接
    - 点击时，重定向到原始
    - 时效问题
    - 自定义短链接
- 非功能性：
    - 可用性
    - 性能：最小延迟
    - 安全性：不可猜测
- 扩展
    - 量分析：发生次数
    - rpc访问


2. 系统接口定义：希望系统做什么？产出api
- createUrl(api_dev_key, original_url, custom_url, user_name, expire_date) 
- deleteUrl(api_dev_key, url_key)
3. 粗略估算：估算规模，产出风险点
- 大量读：读写比100：1， 
- 网络估算：每月5亿个短url创建， qps=500 million / (30 days * 24 hours * 3600seconds)  = ~ 200URLs/s
- 存储估算：存储5年，每个对象500字节，15tb
- 带宽估算：每秒20k url 小于10M
- 内存估算：二八原则估算需要内存 100G


4. 定义数据模型：产出表设计（领域模型）
- 需要存储十亿条数据
- 存储对象小于1k
- 记录之间没有关系
- 读量大

- url映射信息表url_info
- 用户表user_info

5. 高级设计：识别模块，产出类图（功能性设计）
- 这个 URL 的最后六个字符是我们要生成的短键。
- encodeing url
clients--> application server(db) <--- key gen service(key_db)

6. 细节设计：对模块进行设计，对不同的方案的tradeoff。
- clients --> lb --> application server --> lb --> cache servers
                                               --> db


- 扩展数据库：基于范围分区：url第一个字母
            基于哈希分区：url的哈希值
- cache  ： memocache, redis 


7. 解决瓶颈: 找到风险，做好预案（非功能性设计）

- 条目应该永远存在还是应该被清除？如果达到用户指定的过期时间，链接会发生什么？
- 负载策略：最初，我们可以使用简单的循环方法，在后端服务器之间平均分配传入请求。这个 LB 实现简单，不会引入任何开销。这种方法的另一个好处是，如果服务器死了，LB 将把它从轮换中取出，并停止向它发送任何流量.





# 待整理 //todo



> * 如果让你实现一个线程安全的队列，你会怎么实现。
> * 如何从 0 到 1 设计一个类似 Dubbo 的 RPC 框架？
> * 如果让你实现一个 mq，怎么样保证消息不丢失

```plain
 手撕代码：按层次遍历二叉树？
```

统计 100G 的 ip 文件中出现 ip 次数最多的 100 个 ip
 5 台服务器如何选出 leader(选举算法)

4 亿个 int 数，如何找出重复的数（用 hash 方法，建一个 2 的 32 次 方个 bit 的 hash 数组，每取一个 int 数，可 hash 下 2 的 32 次方找到它在 h ash 数组中的位置，然后将 bit 置 1 表示已存在）

4 亿个 url，找出其中重复的（考虑内存不够，通过 hash 算法，将 url 分配到 1000 个文件中，不同的文件间肯定就不会重复了，再分别找出重复 的） 有 1 万个数组，每个数组有 1000 个整数，每个数组都是降序的，从中找出 最大的 N 个数，N<1000

113. LinkedHashmap 的底层实现


微博关注的设计 

京东

滴滴的设计

微信的一些设计 

订单、促销系统





# 如果让你实现一个线程安全的队列，你会怎么实现。

>  实现一个线程安全的队列，可以采用多种设计思路，这里我们采用环形数组和两个指针的方式来设计，并使用Java中的 ReentrantLock 和 Condition 来保证并发访问的安全性。以下是详细的设计思路及代码实现：

## 设计思路:

1. 数据结构选择：我们将使用一个固定大小的环形数组来存储队列元素，这样能够避免在队列满时进行扩容操作，提高性能。
2. 生产者与消费者模型：
   生产者（put）：当队列未满时，将元素放入队列尾部；如果队列已满，则阻塞等待。
   消费者（take）：当队列非空时，从队列头部取出元素；如果队列为空，则阻塞等待。
3. 并发控制：
   使用 java.util.concurrent.locks.ReentrantLock 对象对整个队列进行锁定，确保同一时间只有一个线程能执行入队或出队操作。
   使用 lock.newCondition() 创建两个条件变量 notEmpty 和 notFull 分别用于通知生产者线程可以继续入队和消费者线程可以继续出队。
4. 环形数组处理：
   使用两个索引 tail 和 head 分别表示队列的末尾和头部。当队列元素被消费后，head 向前移动；当新元素被加入时，tail 向前移动。
   当 tail 追上 head 时，意味着队列已满；当 head 追上 tail 并重叠时，意味着队列为空。

## 实现代码 

``` java
import java.util.concurrent.locks.Condition;
import java.util.concurrent.locks.ReentrantLock;

public class ThreadSafeCircularQueue<E> {
    private final Object[] items;
    private int tail;
    private int head;
    private final int capacity;
    private final ReentrantLock lock;
    private final Condition notFull;
    private final Condition notEmpty;

    public ThreadSafeCircularQueue(int capacity) {
        this.capacity = capacity;
        this.items = new Object[capacity];
        this.tail = 0;
        this.head = 0;
        this.lock = new ReentrantLock();
        this.notFull = lock.newCondition();
        this.notEmpty = lock.newCondition();
    }

    public void put(E item) throws InterruptedException {
        lock.lock();
        try {
            while (isFull()) {
                notFull.await();
            }
            items[tail] = item;
            if (++tail == capacity) {
                tail = 0;
            }
            notEmpty.signal();
        } finally {
            lock.unlock();
        }
    }

    public E take() throws InterruptedException {
        lock.lock();
        try {
            while (isEmpty()) {
                notEmpty.await();
            }
            @SuppressWarnings("unchecked")
            E item = (E) items[head];
            items[head] = null; // Help GC
            if (++head == capacity) {
                head = 0;
            }
            notFull.signal();
            return item;
        } finally {
            lock.unlock();
        }
    }

    private boolean isFull() {
        return (tail + 1) % capacity == head;
    }

    private boolean isEmpty() {
        return tail == head;
    }
}

```



# coupang： 外卖中订单和骑手如何匹配？

1. 30秒内接收到的订单要匹配到骑手
2. 骑手上线时才知道在哪个区域，可跨区派送
3. 如果匹配订单和骑手
4. 骑手掉线如何处理
5. 以问题方案的形式产出
6. 订单mq可分区，骑手mq不可



# 滴滴派单算法

[【技术分享】浅谈滴滴派单算法 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/82386733)



# 如何实现O2O平台日订单500万？



## 项目背景介绍：

这是一个小程序点餐平台，用户在小程序上点餐并支付完成后，订单会先落到订单库，然后进一步推送到门店的收银系统；收银系统接单后，推送给后厨系统进行生产；同时返回小程序取餐码，用户可以凭取餐码去门店取餐或收取外卖。

结合以往的经验，以及这次的促销力度，我们预计在高峰时，前端小程序请求将会达到每秒 10 万 QPS，并且预计首日的订单数量会超过 500 万。在这种高并发的情况下，我们为了保证用户的体验，系统整体的可用性要达到 99.99%。

## 具体架构：

![image-20240408075831716](https://p.ipic.vip/c7bkcs.png)

详细描述：

1. 小程序前端通过 Nginx  网关，访问小程序服务端；
2. 小程序服务端会调用一系列的基础服务，完成相应的请求处理，包括门店服务、会员服务、商品服务、订单服务、支付服务等，每个服务都有自己独立的数据库和 Redis 缓存；
3. 订单服务接收到新订单后，先在本地数据库落地订单，然后通过 MQ 同步订单给 OMS 履单中心；
4. 门店的收银系统通过 HTTP  远程访问云端的 OMS 履单中心，拉取新订单，并返回取餐码给 OMS，OMS  再调用小程序订单服务同步取餐码；
5. 小程序前端刷新页面，访问服务端获得取餐码，然后用户可以根据取餐码到门店取餐或等待外卖。



## 高可用系统改造点：

![image-20240408080152082](https://p.ipic.vip/v7c4lf.png)

这次活动的促销力度很大，高峰期流量将达到平时的数十倍，这就要求系统能够在高并发的场景下，保证高可用性。基于访问量、日订单量和可用性的指标，我们对原有系统进行了一系列改造，最终顺利地实现了首日 500 万订单，以及在大促期间，系统 4 个 9 的可用性目标。

1. 前端接入改造： 

   - 小程序端的 CDN 优化；

   - 切换Nginx 负载均衡

   之前是直接利用云服务商提供的  LB，它只有简单的负载均衡能力。为了能应对这次的高并发流量，独立搭建了数十台的 Nginx 集群，集群除了负载均衡，还提供限流支持，如果 QPS 总数超过了 10 万，前端的访问请求将会被丢弃掉。另外，Nginx  在这里还有一个好处，就是可以实时提供每个接口的访问频率和网络带宽占用情况，能够起到很好的接入层监控功能。

2. 应用和服务的水平扩展

   小程序服务端的应用和服务都是无状态的，可以水平扩展。这里的基础服务是 Java 开发的，原来是用虚拟机方式部署的，现在我们把基础服务全部迁移到了容器环境，这样在提升资源利用率的同时，也更好地支持了基础服务的弹性扩容。

3. 订单的水平分库

   高峰期对订单库的操作比较频繁，大约6-7次的写操作，包括创建新订单和订单状态的变化 ；订单的读操作，依赖于一主多从的部署，做到了读写分离。

   负责写入的主库只有一个，所以需要对订单主库进行水平分库操作，扩充主库的实例数。分库的方式也比较简单，通过订单ID取模进行分库，基于进程内的sharing-jdbc，实现数据库的自动路由。

   实践：springboot与sharing-jdbc实现mysql数据库的自动路由，给出整合实例，给出基于订单号水平分库的实例代码

4. 主动通知，避免轮询。

   问题：前端小程序通过轮询的方式，获取取餐码，10s轮询一次，长时间轮询对服务器有很大的压力。同样商户的收银系统也是通过轮询的方式拉取OMS新订单， 这样收银系统有上万个，对OMS系统造成很大的压力。

   <u>建立消息推送中心</u>：收银系统通过<u>socket方式</u>，与消息推送中心建立长连接。当OMS系统接收到前台的新订单，会发送消息到消息推送中心。然后收银系统就可以实时获取通知，再去OMS拉取新的订单信息。为避免消息中心出现问题，收银系统拿不到新订单，收银系统还<u>继续用轮询</u>的方式查询OMS系统，但<u>频率降低</u>到1分钟。

   <u>websokcet</u>：小程序则通过websokcet的方式与消息推送中心保持长连接，这样可以实时的获取后端推送的信息。

   

5. 缓存的使用

   查询新订单：当收银系统向OMS查询新订单时，OMS不是直接从数据库进行查询，而是从缓存中查询订单信息。

   ``` mermaid
   flowchart LR
       A(收银系统) -->|查询订单| B(OMS)
       B --> C(Redis Cache)
   ```

   

   菜单和商品数据：在商品服务中，菜单和商品数据也会放入到redis中，缓存查询压力 。每天凌晨，模拟小程序遍历访问前端每个商品数据，实现对缓存的预刷新。

   

























